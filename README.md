# SKN06-3rd-2Team
ê°•ì±„ì—°, ê¹€ë™ëª…, ë°•ì°½ê·œ, ë°±í•˜ì€, í™ì¤€

## <í‘í‘ìš”ë¦¬ì‚¬>

### íŒ€ì›

| ê°•ì±„ì—° | ê¹€ë™ëª…| ë°•ì°½ê·œ | ë°±í•˜ì€| í™ì¤€ |
| --- | --- | --- | --- | --- |
| <> | <> | <> | ![ê°•ë¡ì´í˜•](https://github.com/user-attachments/assets/1e278c6c-7b68-4a57-8a6f-4097f5faa2c2)
 | <> |
|ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”|ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|

</br>

### RAG ê¸°ë°˜ - ë§›ì§‘ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)![small BR](https://github.com/user-attachments/assets/f5c8c9cf-e5a7-41ad-acf2-36e3d24f374b)![small BR](https://github.com/user-attachments/assets/516ef109-240f-4142-b0da-dbe3ed72ce9e)



### ğŸ´ ê°œë°œ ê¸°ê°„

2024.12.23 ~ 2024.12.26 (ì´ 3ì¼)

### ğŸ´ ê°œìš”

ìš°ë¦¬ë‚˜ë¼ ìµœì´ˆì˜ ë§›ì§‘ â€œë¸”ë£¨ë¦¬ë³¸â€</br>
"ë„ì‹œë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¸ê³„ì ì¸ ë§›ì§‘ ê°€ì´ë“œë¶: ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´"ì—ê²Œ **ë­ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!**</br>
ì ì´ì œ ë‚˜ë§Œì˜ ë§›ì§‘ì„ ì°¾ì•„ ë– ë‚˜ë³¼ê¹Œìš”?

### ğŸ´ ëª©í‘œ

ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´ ì›¹ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìê°€ ì°¾ëŠ” ë§›ì§‘ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ

</br>


#### ğŸ´ Requirements

pandas == <> </br>
numpy == <> </br>
config == 0.5.1 </br>
langchain == 0.3.13 </br>
chromadb == 0.5.23 </br>

</br></br>

## 1. ë°ì´í„° ì¤€ë¹„ ë° ë¶„ì„

### ğŸ£ 1) ë°ì´í„° ìˆ˜ì§‘

        import requests
        import time

        restaurants = []  # ë°ì´í„° ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±

        total_pages = 576 # ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ì„¤ì •
        url_template = "https://www.bluer.co.kr/api/v1/restaurants?page={page}&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/hal+json",
            "x-requested-with": "XMLHttpRequest"
        }

        # í¬ë¡¤ë§ ì§„í–‰
        for page in range(total_pages):
            url = url_template.format(page=page)
            response = requests.get(url, headers=headers)
    
            if response.status_code == 200:
                data = response.json()
                restaurants.extend(data["_embedded"]["restaurants"])
                print(f"Page {page + 1}/{total_pages} collected successfully.")
            else:
                print(f"Failed to fetch page {page + 1}. Status code: {response.status_code}")

        # ê° ìš”ì²­ ì‚¬ì´ì— ì‹œê°„ ê°„ê²© ì¶”ê°€
            time.sleep(2)  # 2ì´ˆ ê°„ê²©

        # ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥
        import json
        with open("restaurants.json", "w", encoding="utf-8") as f:
            json.dump(restaurants, f, ensure_ascii=False, indent=4)

        print("ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ. JSON íŒŒì¼ë¡œ ì €ì¥ë¨.")


### ğŸ”ª 2) ë¶ˆí•„ìš” ì¹¼ëŸ¼ ì‚­ì œ ë° ì •ê·œí™”
> ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’, TMI (ìœ„ë„/ê²½ë„, ì—…ì£¼ëª…) ë“±
> 

```

# ì „ì²˜ë¦¬

import pandas as pd
from pandas import json_normalize

# JSON íŒŒì¼ ì½ê¸°
with open("restaurants.json", "r", encoding="utf-8") as f:
    restaurants = json.load(f)

# Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(restaurants)

# ê¸°ì¡´ ì „ì²˜ë¦¬ ì½”ë“œ ì ìš©
# í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
columns_to_drop = ["createdDate", "id", "timeInfo", "gps", "tags", "status", "bookStatus",
                   "buzUsername", "business", "pageView", "brandMatchStatus", "brandRejectReason",
                   "orderDescending", "foodTypeDetails", "countEvaluate", "bookmark", "features",
                   "feature107", "brandBranches", "foodTypes", "brandHead", "firstImage", "firstLogoImage"]
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Nested JSON ì»¬ëŸ¼ ì •ê·œí™”
nested_columns = ["headerInfo", "defaultInfo", "statusInfo", "juso", "review", "etcInfo","_links"]
for column in nested_columns:
    if column in df.columns:
        n = pd.json_normalize(df[column])  # ì •ê·œí™”
        n.columns = [f"{column}_{subcol}" for subcol in n.columns]  # ì—´ ì´ë¦„ ì ‘ë‘ì‚¬ ì¶”ê°€
        df = pd.concat([df.drop(columns=[column]), n], axis=1)  # ê¸°ì¡´ ì—´ ì‚­ì œ í›„ ê²°í•©

# í•„ìš” ì—†ëŠ” sub_columns ì œê±°
sub_columns_to_drop = ["headerInfo_nickname", "headerInfo_year", "headerInfo_ribbonTypeByOrdinal",
                       "defaultInfo_websiteFacebook", "statusInfo_storeType", "statusInfo_openEra",
                       "statusInfo_newOpenDate", "juso_roadAddrPart2", "juso_jibunAddr", "juso_zipNo",
                       "juso_admCd", "juso_detBdNmList", "juso_zone2_1", "juso_zone2_2", "juso_map_1",
                       "juso_map_2", "review_readerReview", "review_businessReview", "review_editorReview",
                       "etcInfo_toilet", "etcInfo_toiletEtc", "etcInfo_chain", "etcInfo_close", "etcInfo_renewal",
                       "etcInfo_appYn", "etcInfo_projectNo", "etcInfo_reviewerRecommend", "etcInfo_onlySiteView",
                       "etcInfo_history", "etcInfo_mainMemo", "_links_self.href", "_links_restaurant_href",
                       "_links_restaurant_templated", "_links_childrenRestaurants.href", "_links_childrenRestaurants.templated"
                       "_links_evaluates.href", "_links_relativeBusinessOrder.href", "_links_parentRestaurant.href",
                       "_links_parentRestaurant.templated", "_links_reports.href", "_links"
                       ""]
df = df.drop(columns=[col for col in sub_columns_to_drop if col in df.columns])

# ë°ì´í„° ì €ì¥
df.to_csv("nested_all_restaurants.csv", index=False)
print("ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. CSV íŒŒì¼ ì €ì¥ë¨.")


```
### ğŸ¡ 3) í˜•ì‹ ì¼ì¹˜í™”
> ë…„ë„, ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ ê¸°ì… ì—¬ë¶€, ê²°ì¸¡ì¹˜ í‘œê¸°
>

```

import numpy as np
import re

# ë¹ˆì¹¸ì´ë‚˜ "ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°ëœ ì¹¸ì„ Noneìœ¼ë¡œ ë³€ê²½
df.replace(["", "ì—†ìŒ"], None, inplace=True)

# foodDetailTypesì˜ ë¦¬ìŠ¤íŠ¸ í•´ì œ
# ê° í–‰ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ë¼ë©´ ì´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³‘í•©)
if "foodDetailTypes" in df.columns:
    df["foodDetailTypes"] = df["foodDetailTypes"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)

if "headerInfo_ribbonType" in df.columns:
    df["headerInfo_ribbonType"] = df["headerInfo_ribbonType"].map(ribbon_mapping)

# ì›¹ì‚¬ì´íŠ¸ ì—´ í†µí•©
if "defaultInfo_website" in df.columns and "defaultInfo_websiteInstagram" in df.columns:
    def merge_websites(row):
        websites = [site for site in [row.get("defaultInfo_website"), row.get("defaultInfo_websiteInstagram")] if site]
        return ", ".join(websites) if websites else None

    df["defaultInfo_website_combined"] = df.apply(merge_websites, axis=1)
    df.drop(columns=["defaultInfo_website", "defaultInfo_websiteInstagram"], inplace=True)
    df.rename(columns={"defaultInfo_website_combined": "defaultInfo_website"}, inplace=True)

# statusInfo_openDate í˜•ì‹ ë³€í™˜ (ë…„ë„ 4ìë¦¬ë¡œ ì¶”ì¶œ í›„ "2024ë…„" í˜•ì‹ìœ¼ë¡œ í‘œê¸°)
def extract_year_with_suffix(date):
    if isinstance(date, str):
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìˆ«ì 4ìë¦¬ ì¶”ì¶œ
        match = re.search(r'\d{4}', date)
        if match:
            return f"{int(match.group(0))}ë…„"  # "2024ë…„" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return None

if "statusInfo_openDate" in df.columns:
    df["statusInfo_openDate"] = df["statusInfo_openDate"].apply(extract_year_with_suffix)

# ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
df.to_csv("cleaned_all_restaurants.csv", index=False)
print("ìˆ˜ì •ëœ ìµœì¢… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ!")


```

### ğŸ¥© 4) ì‚°ì¶œë¬¼ ì •ë¦¬
> ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë°ì´í„° : blueRibbon.csv </br>
> í•„ìš”ì—†ëŠ” column ì œê±° í›„ ë°ì´í„° : nested_all_restaurants.csv </br>
> ì •ê·œí™” ë° ì „ì²˜ë¦¬ í›„ ë°ì´í„° : cleaned_all_restaurants.csv </br>

</br></br>

## 2. ëª¨ë¸ë§

### ğŸ– 1) embedding_vector ìƒì„±
> text_splitter, embeddings ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í•´ ë° ì €ì¥
> ê²°ê³¼ : vector_storeì— 39074 ê°œì˜ ë¬¸ì„œ ìƒì„±

```
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥
data = pd.read_csv('data/cleaned_all_restaurants.csv')


# ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ë„ë¡ ë¬¸ì„œí™”

documents = []
for i, row in data.iterrows():
    # í…ìŠ¤íŠ¸ ë‚´ìš© (ê° í–‰ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì·¨ê¸‰)
    page_content = "\n".join([f"{col}: {val}" for col, val in row.items()])
    
    # Document ìƒì„±
    doc = Document(page_content=page_content)
    documents.append(doc)

print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë¶„ë¦¬
splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name=MODEL_NAME, 
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP
)
splited_docs = splitter.split_documents(documents)

print("ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ:", len(splited_docs), end='\n\n')


# Vector store ì €ì¥
embedding_model = OpenAIEmbeddings(
    model=EMBEDDING_NAME
)

# Persist directory ì—†ëŠ” ê²½ìš° ìƒì„±
if not os.path.exists(PERSIST_DIRECTORY):
    os.makedirs(PERSIST_DIRECTORY)

# ì—°ê²° + document ì¶”ê°€
vector_store = Chroma.from_documents(
    documents= splited_docs,
    embedding=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)

print("vecor_storeì— splited_docs ì €ì¥ì™„ë£Œ")
```

### ğŸ– 2) configë¡œë¶€í„° ì„¤ì • ê°’ ì…ë ¥
> chunk_size : 500
> chunk_overlap : 100
> 
```
# setting

CHUNK_SIZE = config.chunk_size
CHUNK_OVERLAP = config.chunk_overlap

MODEL_NAME  = config.model_name
EMBEDDING_NAME = config.embedding_name

COLLECTION_NAME = config.collection_name
PERSIST_DIRECTORY = config.persist_directory

```

### ğŸ– 3) vector_store ì— ì €ì¥
```
data = pd.read_csv("final_merged_result.csv")


# ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ë„ë¡ ë¬¸ì„œí™”
data.fillna("", inplace=True)  # NaN ê°’ ì²˜ë¦¬

documents = []
for i, row in data.iterrows():
    # í…ìŠ¤íŠ¸ ë‚´ìš© (ê° í–‰ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì·¨ê¸‰)
    page_content = "\n".join([f"{col}: {val}" for col, val in row.items()])
    metadata = row.to_dict()
    # Document ìƒì„±
    doc = Document(page_content=page_content, metadata=metadata)
    
    documents.append(doc)

print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë¶„ë¦¬
splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name=MODEL_NAME, 
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP
)
splited_docs = splitter.split_documents(documents)

print("ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ:", len(splited_docs), end='\n\n')


# Vector store ì €ì¥
embedding_model = OpenAIEmbeddings(
    model=EMBEDDING_NAME
)

# Persist directory ì—†ëŠ” ê²½ìš° ìƒì„±
if not os.path.exists(PERSIST_DIRECTORY):
    os.makedirs(PERSIST_DIRECTORY)

# ì—°ê²° + document ì¶”ê°€
vector_store = Chroma.from_documents(
    documents= splited_docs,
    embedding=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)

print("vecor_storeì— splited_docs ì €ì¥ì™„ë£Œ")
```

> ì €ì¥ëœ ë‚´ìš© í™•ì¸
    
```
COLLECTION_NAME = "bluer_db_openai"
PERSIST_DIRECTORY = "vector_store/chroma/bluer_db"
EMBEDDING_MODEL_NAME = "text-embedding-3-small"
embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
MODEL_NAME = 'gpt-4o-mini'

# vector store ì—°ê²°
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)

# ì €ì¥ëœ ë°ì´í„° ë‚´ìš© í™•ì¸
documents = vector_store._collection.get()['documents']
metadatas = vector_store._collection.get()['metadatas']

print(f"Documents: {documents[:5]}") 
print(f"Metadatas: {metadatas[:5]}")
print(vector_store._collection.count())
```
</br></br>
## 3. GPT ëª¨ë¸, Prompt, Retriever ìƒì„±
```
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)


# GPT Model ìƒì„±
model = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0 
)


# Retriever ìƒì„± - "Map Reduce" ë°©ì‹ - ë” ì •í™•í•œ ë‹µë³€
retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={"k":5, "fetch_k":10, "lambda_mult":0.5}
)


# # Prompt Template ìƒì„±
# prompt_template = ChatPromptTemplate(
#     [
#         ("system", "ë‹¹ì‹ ì€ í•œêµ­ì˜ ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ìì„¸íˆ ë‹µí•´ì£¼ì„¸ìš”."),
#         MessagesPlaceholder("history"), 
#         ("human", "{query}")
#     ]
# )

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a professinal for blue ribbon survey in korea. please reply correct anwser from exact information."),
    ("human", "{question}")
])


# Chain ìƒì„±

# retrieval_qa = RetrievalQA.from_chain_type(
#     llm=model,
#     retriever=retriever,
#     return_source_documents=True,  # Include source documents in response
# )

chain = ({'content': retriever, 'question':RunnablePassthrough()} | prompt_template | model | StrOutputParser() )

# response = retrieval_qa({"query": QUERY})

# print("ì‘ë‹µ:", response["result"])
```

</br></br>
## ğŸ˜‹ ëª¨ë¸ í‰ê°€

| ì§ˆë¬¸ | ì„œì´ˆë™ì— ê°€ì¡±ê³¼ í•¨ê»˜ ê°ˆë§Œí•œ ì‹ë‹¹ì„ ì†Œê°œí•´ì¤˜ |
| ------- |-------|
| ì‘ë‹µ | ~~ |
| ------- |-------|
| ì§ˆë¬¸ | ì²­ë‹´ë™ì— ë¸”ë£¨ë¦¬ë³¸ 2ê°œ ì´ìƒì¸ ì¼ì‹ë‹¹ì„ ì†Œê°œí•´ì¤˜|
| ------- |-------|
| ì‘ë‹µ | ~~ |

###  ìµœê³  ì„±ëŠ¥ ëª¨ë¸![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)

ğŸ† <-------------------->

</br>

### ëª¨ë¸ ì €ì¥
>


```

```

</br></br>

## íŒ€ì› íšŒê³ 
ê°•ì±„ì—°
> 
>
ê¹€ë™ëª…
>

ë°•ì°½ê·œ
>

ë°±í•˜ì€
>

í™ì¤€
>
