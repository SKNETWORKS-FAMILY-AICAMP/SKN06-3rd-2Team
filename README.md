# SKN06-3rd-2Team
ê°•ì±„ì—°, ê¹€ë™ëª…, ë°•ì°½ê·œ, ë°±í•˜ì€, í™ì¤€

## <íŒ€ëª…>

### íŒ€ì›

| ê°•ì±„ì—° | ê¹€ë™ëª…| ë°•ì°½ê·œ | ë°±í•˜ì€| í™ì¤€ |
| --- | --- | --- | --- | --- |
| <> | <> | <> | <> | <> |
|ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”|ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|

</br>

### RAG ê¸°ë°˜ - ë§›ì§‘ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)![small BR](https://github.com/user-attachments/assets/f5c8c9cf-e5a7-41ad-acf2-36e3d24f374b)![small BR](https://github.com/user-attachments/assets/516ef109-240f-4142-b0da-dbe3ed72ce9e)



### ğŸ´ ê°œë°œ ê¸°ê°„

2024.12.23 ~ 2024.12.26 (ì´ 3ì¼)

### ğŸ´ ê°œìš”

ìš°ë¦¬ë‚˜ë¼ ìµœì´ˆì˜ ë§›ì§‘ â€œë¸”ë£¨ë¦¬ë³¸â€</br>
"ë„ì‹œë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¸ê³„ì ì¸ ë§›ì§‘ ê°€ì´ë“œë¶: ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´"ì—ê²Œ **ë­ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!**</br>
ì ì´ì œ ë‚˜ë§Œì˜ ë§›ì§‘ì„ ì°¾ì•„ ë– ë‚˜ë³¼ê¹Œìš”?

### ğŸ´ ëª©í‘œ

ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´ ì›¹ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìê°€ ì°¾ëŠ” ë§›ì§‘ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ

</br>

#### ğŸ´ Stacks

![Discord](https://img.shields.io/badge/discord-5865F2?style=for-the-badge&logo=discord&logoColor=white)

![Python](https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Numpy](https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=NumPy&logoColor=white)
![pandas](https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
</br>
![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=Git&logoColor=white)
![Github](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)
<br/> <>
<br/> <>
<br/> <>
<br/> <>
<br/> <>
<br/> <>


#### ğŸ´ Requirements

pandas == <> <br/>
numpy == <> <br/>
<> == <> <br/>

## ë°ì´í„° ì¤€ë¹„ ë° ë¶„ì„

### ğŸ£ 1. ë°ì´í„° ìˆ˜ì§‘

        import requests
        import time

        restaurants = []  # ë°ì´í„° ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±

        total_pages = 576 # ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ì„¤ì •
        url_template = "https://www.bluer.co.kr/api/v1/restaurants?page={page}&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/hal+json",
            "x-requested-with": "XMLHttpRequest"
        }

        # í¬ë¡¤ë§ ì§„í–‰
        for page in range(total_pages):
            url = url_template.format(page=page)
            response = requests.get(url, headers=headers)
    
            if response.status_code == 200:
                data = response.json()
                restaurants.extend(data["_embedded"]["restaurants"])
                print(f"Page {page + 1}/{total_pages} collected successfully.")
            else:
                print(f"Failed to fetch page {page + 1}. Status code: {response.status_code}")

        # ê° ìš”ì²­ ì‚¬ì´ì— ì‹œê°„ ê°„ê²© ì¶”ê°€
            time.sleep(2)  # 2ì´ˆ ê°„ê²©

        # ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥
        import json
        with open("restaurants.json", "w", encoding="utf-8") as f:
            json.dump(restaurants, f, ensure_ascii=False, indent=4)

        print("ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ. JSON íŒŒì¼ë¡œ ì €ì¥ë¨.")


### ğŸ”ª 2. ë¶ˆí•„ìš” ì¹¼ëŸ¼ ì‚­ì œ ë° ì •ê·œí™”
> ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’, TMI (ìœ„ë„/ê²½ë„, ì—…ì£¼ëª…) ë“±
> 

```

#### 1. ì „ì²˜ë¦¬

import pandas as pd
from pandas import json_normalize

# JSON íŒŒì¼ ì½ê¸°
with open("restaurants.json", "r", encoding="utf-8") as f:
    restaurants = json.load(f)

# Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(restaurants)

# ê¸°ì¡´ ì „ì²˜ë¦¬ ì½”ë“œ ì ìš©
# í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
columns_to_drop = ["createdDate", "id", "timeInfo", "gps", "tags", "status", "bookStatus",
                   "buzUsername", "business", "pageView", "brandMatchStatus", "brandRejectReason",
                   "orderDescending", "foodTypeDetails", "countEvaluate", "bookmark", "features",
                   "feature107", "brandBranches", "foodTypes", "brandHead", "firstImage", "firstLogoImage"]
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Nested JSON ì»¬ëŸ¼ ì •ê·œí™”
nested_columns = ["headerInfo", "defaultInfo", "statusInfo", "juso", "review", "etcInfo","_links"]
for column in nested_columns:
    if column in df.columns:
        n = pd.json_normalize(df[column])  # ì •ê·œí™”
        n.columns = [f"{column}_{subcol}" for subcol in n.columns]  # ì—´ ì´ë¦„ ì ‘ë‘ì‚¬ ì¶”ê°€
        df = pd.concat([df.drop(columns=[column]), n], axis=1)  # ê¸°ì¡´ ì—´ ì‚­ì œ í›„ ê²°í•©

# í•„ìš” ì—†ëŠ” sub_columns ì œê±°
sub_columns_to_drop = ["headerInfo_nickname", "headerInfo_year", "headerInfo_ribbonTypeByOrdinal",
                       "defaultInfo_websiteFacebook", "statusInfo_storeType", "statusInfo_openEra",
                       "statusInfo_newOpenDate", "juso_roadAddrPart2", "juso_jibunAddr", "juso_zipNo",
                       "juso_admCd", "juso_detBdNmList", "juso_zone2_1", "juso_zone2_2", "juso_map_1",
                       "juso_map_2", "review_readerReview", "review_businessReview", "review_editorReview",
                       "etcInfo_toilet", "etcInfo_toiletEtc", "etcInfo_chain", "etcInfo_close", "etcInfo_renewal",
                       "etcInfo_appYn", "etcInfo_projectNo", "etcInfo_reviewerRecommend", "etcInfo_onlySiteView",
                       "etcInfo_history", "etcInfo_mainMemo", "_links_self.href", "_links_restaurant_href",
                       "_links_restaurant_templated", "_links_childrenRestaurants.href", "_links_childrenRestaurants.templated"
                       "_links_evaluates.href", "_links_relativeBusinessOrder.href", "_links_parentRestaurant.href",
                       "_links_parentRestaurant.templated", "_links_reports.href", "_links"
                       ""]
df = df.drop(columns=[col for col in sub_columns_to_drop if col in df.columns])

# ë°ì´í„° ì €ì¥
df.to_csv("nested_all_restaurants.csv", index=False)
print("ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. CSV íŒŒì¼ ì €ì¥ë¨.")


```
### ğŸ¡ 3. í˜•ì‹ ì¼ì¹˜í™”
> ë…„ë„, ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ ê¸°ì… ì—¬ë¶€, ê²°ì¸¡ì¹˜ í‘œê¸°
>

```

import numpy as np
import re

# ë¹ˆì¹¸ì´ë‚˜ "ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°ëœ ì¹¸ì„ Noneìœ¼ë¡œ ë³€ê²½
df.replace(["", "ì—†ìŒ"], None, inplace=True)

# foodDetailTypesì˜ ë¦¬ìŠ¤íŠ¸ í•´ì œ
# ê° í–‰ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ë¼ë©´ ì´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³‘í•©)
if "foodDetailTypes" in df.columns:
    df["foodDetailTypes"] = df["foodDetailTypes"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)

if "headerInfo_ribbonType" in df.columns:
    df["headerInfo_ribbonType"] = df["headerInfo_ribbonType"].map(ribbon_mapping)

# ì›¹ì‚¬ì´íŠ¸ ì—´ í†µí•©
if "defaultInfo_website" in df.columns and "defaultInfo_websiteInstagram" in df.columns:
    def merge_websites(row):
        websites = [site for site in [row.get("defaultInfo_website"), row.get("defaultInfo_websiteInstagram")] if site]
        return ", ".join(websites) if websites else None

    df["defaultInfo_website_combined"] = df.apply(merge_websites, axis=1)
    df.drop(columns=["defaultInfo_website", "defaultInfo_websiteInstagram"], inplace=True)
    df.rename(columns={"defaultInfo_website_combined": "defaultInfo_website"}, inplace=True)

# statusInfo_openDate í˜•ì‹ ë³€í™˜ (ë…„ë„ 4ìë¦¬ë¡œ ì¶”ì¶œ í›„ "2024ë…„" í˜•ì‹ìœ¼ë¡œ í‘œê¸°)
def extract_year_with_suffix(date):
    if isinstance(date, str):
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìˆ«ì 4ìë¦¬ ì¶”ì¶œ
        match = re.search(r'\d{4}', date)
        if match:
            return f"{int(match.group(0))}ë…„"  # "2024ë…„" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return None

if "statusInfo_openDate" in df.columns:
    df["statusInfo_openDate"] = df["statusInfo_openDate"].apply(extract_year_with_suffix)

# ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
df.to_csv("cleaned_all_restaurants.csv", index=False)
print("ìˆ˜ì •ëœ ìµœì¢… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ!")


```

### ğŸ¥© 4. ì‚°ì¶œë¬¼ ì •ë¦¬
> ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë°ì´í„° : blueRibbon.csv </br>
> í•„ìš”ì—†ëŠ” column ì œê±° í›„ ë°ì´í„° : nested_restaurants.csv </br>
> ì •ê·œí™” ë° ì „ì²˜ë¦¬ í›„ ë°ì´í„° : cleaned_all_restaurants.csv </br>

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

#### ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥
```
import joblib
import os

os.makedirs('models', exist_ok=True)
joblib.dump(
    preprocessor_pipeline,     # ì €ì¥í•  ëª¨ë¸/ì „ì²˜ë¦¬ê¸°
    "models/preprocessor.pkl"  # ì €ì¥ê²½ë¡œ. pickleë¡œ ì €ì¥ëœë‹¤.
)
```
## ëª¨ë¸ë§

### âœ”ï¸ ëª¨ë¸ ì„ ì •í•˜ê¸°

ë°ì´í„°ì™€ ì–´ìš¸ë¦¬ëŠ” 7ê°œì˜ ëª¨ë¸ë“¤ì€ ë½‘ì•„ ì–´ë–¤ ëª¨ë¸ì´ ì í•©í• ì§€ í™•ì¸í•´ ë³´ê¸°ë¡œ í–ˆë‹¤.

- í‰ê°€

```
  from tqdm import tqdm

  from sklearn.linear_model import LogisticRegression
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.ensemble import GradientBoostingClassifier
  from xgboost import XGBClassifier, plot_importance
  from sklearn.svm import SVC
  from sklearn.neighbors import KNeighborsClassifier

  import matplotlib.pyplot as plt
  models = {
      # Logistic Regression model
      "Logistic Regression": LogisticRegression(),
      # Decision Tree model
      "Decision Tree Classifier": DecisionTreeClassifier(),
      # Random Forest model
      "Random Forest": RandomForestClassifier(),
      # Gradient Boosting model
      "Gradient Boosting": GradientBoostingClassifier(),
      # XGBoost model
      "XGBoost": XGBClassifier(),
      # SVM(Support Vector Machine)
      "SVC": SVC(),
      # KNN(K-Nearest Neighbors)
      "KNeighborsClassifier": KNeighborsClassifier(),
  }


  for name, model in tqdm(models.items(), desc="Training Models", total=len(models)):
      # ëª¨ë¸ í›ˆë ¨
      model.fit(X_train, y_train)
      # ëª¨ë¸ í‰ê°€
      score = model.score(X_test, y_test)
      # ëª¨ë¸ ê²€ì¦
      model_pred = model.predict(X_test)
      # ëª¨ë¸ ì •í™•ë„
      tqdm.write(f">>> {name} : ì •í™•ë„ {score:.2%}\n")

```

- ê²°ê³¼

```python
>>> Logistic Regression : ì •í™•ë„ 87.90%

>>> Decision Tree Classifier : ì •í™•ë„ 94.02%

>>> Random Forest : ì •í™•ë„ 95.65%

>>> Gradient Boosting : ì •í™•ë„ 96.15%

>>> XGBoost : ì •í™•ë„ 96.74%

>>> SVC : ì •í™•ë„ 84.20%

>>> KNeighborsClassifier : ì •í™•ë„ 90.47%
```

#### â­ ì„ ì • ê²°ê³¼

- LogisticRegression
- DecisionTreeClassifier (âœ”ï¸) - ê¹€ë™ëª…
- RandomForestClassifier (âœ”ï¸) - ì„ì—°ê²½
- GradientBoostingClassifier (âœ”ï¸) - ë°•ìœ ë‚˜
- xgboost (âœ”ï¸) - ê³µì¸ìš©
- SVC
- KNeighborsClassifier

7ê°œì˜ ëª¨ë¸ ì¤‘ 4ê°œì˜ ëª¨ë¸ì´ ìš°ìˆ˜í•œ í¸ì´ì—ˆê³ , ê°ì ëª¨ë¸ í•œê°œì”© ë§¡ì•„ì„œ ëª¨ë¸ë§ì„ í•˜ê¸°ë¡œ í–ˆë‹¤.

### âœ”ï¸ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸

#### 1. Decision Tree Classifier : ì •í™•ë„ 93.78%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > criterion: ë…¸ë“œ ë¶„í•  ê¸°ì¤€
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > min_samples_split: ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
  >
  > min_samples_leaf: ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
  >
  > max_features: ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜

  ```

  from sklearn.tree import DecisionTreeClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)



  tree = DecisionTreeClassifier()

  tree.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_tree = tree.predict(X_train)
  y_train_proba_tree= tree.predict_proba(X_train)[:, 1]

  y_test_pred_tree = tree.predict(X_test)
  y_test_proba_tree= tree.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_tree)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Decision Tree (Test Set)")
  plt.show()

  evaluate("Train - Decision Tree", y_train, y_train_pred_tree, y_train_proba_tree)
  evaluate("Test - Decision Tree", y_test, y_test_pred_tree, y_test_proba_tree)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = tree.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Decision Tree")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      'criterion': ['gini', 'entropy'],  # ë…¸ë“œ ë¶„í•  ê¸°ì¤€
      'max_depth': [None, 10, 20, 30],   # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'min_samples_split': [2, 10, 20],  # ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
      'min_samples_leaf': [1, 5, 10],    # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
      'max_features': [None, 'sqrt', 'log2']  # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜
  }

  gs_tree = GridSearchCV(
      estimator=tree,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_tree.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_tree = gs_tree.best_params_
  best_model_tree = gs_tree.best_estimator_

  best_y_pred_tree = best_model_tree.predict(X_test)
  best_y_proba_tree= best_model_tree.predict_proba(X_test)[:, 1]

  ```

#### 2. Random Forest : ì •í™•ë„ 95.65%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > max_features: ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜

  ```
  from sklearn.ensemble import RandomForestClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  rf = RandomForestClassifier()

  rf.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_rf = rf.predict(X_train)
  y_train_proba_rf= rf.predict_proba(X_train)[:, 1]

  y_test_pred_rf = rf.predict(X_test)
  y_test_proba_rf= rf.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_rf)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Random Forest (Test Set)")
  plt.show()

  evaluate("Train - Random Forest", y_train, y_train_pred_rf, y_train_proba_rf)
  evaluate("Test - Random Forest", y_test, y_test_pred_rf, y_test_proba_rf)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = rf.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Random Forest")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      'n_estimators': [100, 200, 300],    # ê²°ì • íŠ¸ë¦¬(Decision Tree)ì˜ ê°œìˆ˜
      'max_depth': [5, 10, 15],           # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'max_features': ['sqrt', 'log2']    # ê° íŠ¸ë¦¬ê°€ í•™ìŠµí•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  íŠ¹ì„±(feature)ì˜ ìˆ˜
  }
  gs_rf = GridSearchCV(
      estimator=rf,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_rf.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_rf = gs_rf.best_params_
  best_model_rf = gs_rf.best_estimator_

  best_y_pred_rf = best_model_rf.predict(X_test)
  best_y_proba_rf= best_model_rf.predict_proba(X_test)[:, 1]

  ```

#### 3. Gradient Boosting : ì •í™•ë„ 96.79%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > learning_rate: í•™ìŠµë¥ 
  >
  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > subsample: ê° íŠ¸ë¦¬ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨

  ```
  from sklearn.ensemble import GradientBoostingClassifier

  # 1. í•™ìŠµ ë° ì˜ˆì¸¡
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  gb = GradientBoostingClassifier()

  gb.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_gb = gb.predict(X_train)
  y_train_proba_gb= gb.predict_proba(X_train)[:, 1]

  y_test_pred_gb = gb.predict(X_test)
  y_test_proba_gb= gb.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_gb)
  plt.figure(figsize=(6,4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - Gradient Boosting (Test Set)")
  plt.show()

  evaluate("Train - Gradient Booting", y_train, y_train_pred_gb, y_train_proba_gb)
  evaluate("Test - Gradient Booting", y_test, y_test_pred_gb, y_test_proba_gb)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = gb.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in Gradient Boosting")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      "n_estimators": [100, 200, 300],  #  ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
      "learning_rate": [0.1],  # í•™ìŠµë¥ 
      "max_depth": [1, 2, 3, 4, 5],  # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      "subsample": [0.5, 0.7],  # ìƒ˜í”Œë§ ë¹„ìœ¨
  }

  gs_gb = GridSearchCV(
      estimator=gb,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_gb.fit(X_train, y_train)

  # 5. Best Model: ìµœì ì˜ í•˜ì´íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“  ëª¨ë¸
  best_param_gb = gs_gb.best_params_
  best_model_gb = gs_gb.best_estimator_

  best_y_pred_gb = best_model_gb.predict(X_test)
  best_y_proba_gb= best_model_gb.predict_proba(X_test)[:, 1]

  ```

#### 4. XGBoost : ì •í™•ë„ 97.19%

- ì£¼ìš” íŒŒë¼ë¯¸í„°

  > max_depth: ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
  >
  > learning_rate: í•™ìŠµë¥ 
  >
  > n_estimators: ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
  >
  > subsample: ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨
  >
  > colsample_bytree: ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ë¹„ìœ¨
  >
  > gamma: ë…¸ë“œ ë¶„í• ì— ëŒ€í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ
  >
  > reg_alpha: L1 ì •ê·œí™”
  >
  > reg_lambda: L2 ì •ê·œí™”

  ```
  from xgboost import XGBClassifier

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  xgb = XGBClassifier()

  xgb.fit(X_train, y_train)

  # 2. ëª¨ë¸ í‰ê°€
  # Train set + Test set í‰ê°€
  y_train_pred_xgb = xgb.predict(X_train)
  y_train_proba_xgb= xgb.predict_proba(X_train)[:, 1]

  y_test_pred_xgb = xgb.predict(X_test)
  y_test_proba_xgb= xgb.predict_proba(X_test)[:, 1]

  # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
  cm_test = confusion_matrix(y_test, y_test_pred_xgb)
  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False)
  plt.xlabel("ì˜ˆì¸¡")
  plt.ylabel("ì •ë‹µ")
  plt.title("Confusion Matrix - XGBoost (Test Set)")
  plt.show()

  evaluate("Train - XGBoost", y_train, y_train_pred_xgb, y_train_proba_xgb)
  evaluate("Test - XGBoost", y_test, y_test_pred_xgb, y_test_proba_xgb)

  # 3. íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë° ì‹œê°í™”
  fi = xgb.feature_importances_
  fi_series = pd.Series(fi, index=df.drop(columns="churn").columns).sort_values(ascending=False)

  # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x=fi_series, y=fi_series.index)
  plt.title("Feature Importances in XGBoost")
  plt.xlabel("Importance")
  plt.ylabel("Feature")
  plt.show()

  # 4. ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ êµ¬í•˜ê¸° - GridSearchCV
  params = {
      "max_depth":[1, 2, 3, 4, 5],            # ê° ê²°ì • íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì„¤ì •
      'learning_rate': [0.1],                 # í•™ìŠµë¥ 
      'n_estimators': [100, 200, 300],        # ë¶€ìŠ¤íŒ… ë‹¨ê³„ì˜ ìˆ˜ = ëª¨ë¸ì´ ìƒì„±í•  íŠ¸ë¦¬ ê°œìˆ˜
      'subsample': [0.5, 0.7],                # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨
      'colsample_bytree': [0.5, 0.7, 1.0],    # ê° íŠ¸ë¦¬ì˜ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” í”¼ì²˜ ë¹„ìœ¨
      'gamma': [0, 0.1],                      # ë…¸ë“œ ë¶„í• ì— ëŒ€í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ
      'reg_alpha': [0],                       # L1 ì •ê·œí™”
      'reg_lambda': [0.1]                     # L2 ì •ê·œí™”
  }
  gs_xgb = GridSearchCV(
      estimator=xgb,
      param_grid=params,
      scoring=scoring,
      refit='accuracy',
      cv=5,
      n_jobs=-1,
  )

  gs_xgb.fit(X_train, y_train)

  # 5. íŠœë‹ : Best Model ì°¾ê¸°
  best_param_xgb = gs_xgb.best_params_
  best_model_xgb = gs_xgb.best_estimator_

  best_y_pred_xgb = best_model_xgb.predict(X_test)
  best_y_proba_xgb= best_model_xgb.predict_proba(X_test)[:, 1]

  ```

| ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•    | Decision Tree Classifier                                                                                                                                                                                                                | Random Forest                                                                                                                                                                                                                                             | Gradient Boosting                                                                                                                                                                                           | XGBoost                                                                                                                                                                                                    |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Confusion Matrix | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-cm.png" alt="image" width="200" height="200"/>                                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-cm.png" width="200" height="200"/>                                                                          | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-cm.png" alt="image" width="200" height="200"/>                                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-cm.png" alt="image" width="200" height="200"/>                                                              |
| ê²°ê³¼             | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%EA%B2%B0%EA%B3%BC.png" width="300" height="150"/>                                                          | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-%EA%B2%B0%EA%B3%BC.png" alt="image" width="300" height="150"/>                                              |
| íŠ¹ì„±ì¤‘ìš”ë„       | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" width="300" height="150"/>                               | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGboost-%ED%8A%B9%EC%84%B1%EC%A4%91%EC%9A%94%EB%8F%84.png" alt="image" width="300" height="150"/>                   |
| í•˜ì´í¼íŒŒë¼ë¯¸í„°   | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="160"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="100"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/gradient-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="150"/> | <img src="https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/XGBoost-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.png" alt="image" width="200" height="150"/> |

### âœ”ï¸ ëª¨ë¸ í‰ê°€

```
# ì—¬ëŸ¬ í‰ê°€ ì§€í‘œ ì„¤ì •
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'auc': make_scorer(roc_auc_score)
}

model_box = pd.DataFrame(columns=['decision_tree', 'random_forest', 'gradient_boosting', 'xgboost'],
                            index = ['accuracy','precision','recall','f1 score','auc'])

def evaluate(title, y_real, y_pred, y_prob):
    acc = accuracy_score(y_real, y_pred)
    pre = precision_score(y_real, y_pred)
    rec = recall_score(y_real, y_pred)
    f1 = f1_score(y_real, y_pred)
    auc = roc_auc_score(y_real, y_prob)

    print(f"======= {title} =======")
    print('Accuracy : {:.6f}'.format(acc)) # ì •í™•ë„ : ì˜ˆì¸¡ì´ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€
    print('Precision : {:.6f}'.format(pre)) # ì •ë°€ë„ : ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì—ì„œ ì •ë‹µì˜ ë¹„ìœ¨
    print('Recall : {:.6f}'.format(rec)) # ì¬í˜„ìœ¨ : ì •ë‹µ ì¤‘ì—ì„œ ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨
    print('F1 score : {:.6f}'.format(f1)) # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ (ì¡°í™”)í‰ê·  - ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì´ ë¹„ìŠ·í• ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    print('auc: {:.6f}'.format(auc))


    score_list = [acc,pre,rec,f1,auc]
    score_box = np.array(score_list)

    return score_box
```
| ëª¨ë¸ ì¢…ë¥˜ | Decision Tree | Random Forest | Gradient Boosting | XGBoost|
|---------|---------------|---------------|-------------------|--------|
| Accuracy | 0.943196| 0.961348|0.974315|0.988392|
| Precision | 0.797872 | 0.957543| 0.954774|0.973059|
| Recall | 0.865385|0.799397| 0.880989|0.953416|
|F1 score | 0.830258|0.871352| 0.916399|0.963137|
|AUC|0.964163|0.993737|0.994633|0.998675|

### âœ”ï¸ ìµœê³  ì„±ëŠ¥ ëª¨ë¸

ğŸ† XGBOOST

<br/>

## ëª¨ë¸ ì €ì¥

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ê° ëª¨ë¸ë³„ best params ë¥¼ í†µí•´ ë§Œë“  best modelë“¤ì„ .pkl íŒŒì¼ë¡œ ì €ì¥.

```
import os
import joblib

directory = 'model/'
os.makedirs(directory, exist_ok=True)

joblib.dump(best_model_tree, os.path.join(directory, 'best_tree.pkl'))
joblib.dump(best_model_rf, os.path.join(directory, 'best_rf.pkl'))
joblib.dump(best_model_gb, os.path.join(directory, 'best_gb.pkl'))
joblib.dump(best_model_xgb, os.path.join(directory, 'best_xgb.pkl'))

# ì €ì¥ëœ ëª¨ë¸ê³¼ íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸°
model_tree = joblib.load('model/best_tree.pkl')
model_rf = joblib.load('model/best_rf.pkl')
model_gb = joblib.load('model/best_gb.pkl')
model_xgb = joblib.load('model/best_xgb.pkl')
```

## Streamlit


![image](https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN06-2nd-4Team/blob/main/report/%EC%8A%A4%ED%8A%B8%EB%A6%BC%EB%A6%BF%20%EC%8B%A4%ED%96%89%20%ED%99%94%EB%A9%B4.png)

## íŒ€ì› íšŒê³ 
ê¹€ë™ëª…
> ëê¹Œì§€ ì—´ì‹¬íˆ ì‘ì—…í•´ì£¼ì‹  íŒ€ì›ë¶„ë“¤ ê°ì‚¬í•´ìš”!
>
ë°•ìœ ë‚˜
> ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì¶©ë¶„íˆ ë‹¤ë¤„ë³¼ ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì°¾ëŠ” ë°ì— ì£¼ì•ˆì ì„ ë‘ì—ˆëŠ”ë° íŒ€ì›ë“¤ê³¼ ê°™ì€ ë°©í–¥ì„±ì„ ê³µìœ í•˜ë©° ì ì ˆí•œ ë°ì´í„°ë¥¼ ì„ ì •í•  ìˆ˜ ìˆì—ˆì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤.
>
> ê·¸ë¦¬ê³  ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê³¼ì •ì„ ë‹¤ í•¨ê»˜í•˜ë©° í•™ìŠµí•´ë³´ìëŠ” ëª©í‘œê°€ ìˆì—ˆëŠ”ë° ë¹„ë¡ ì‹œê°„ì´ ìƒëŒ€ì ìœ¼ë¡œ ê±¸ë¦¬ê¸´ í–ˆì§€ë§Œ ê·¸ë§Œí¼ ìœ ìµí•œ ê²½í—˜ì´ë˜ì§€ ì•Šì•˜ë‚˜ ìƒê°í•©ë‹ˆë‹¤. ì €ëŠ” ê·¸ ê³¼ì •ì—ì„œ ë°©í–¥ì„ ì œì‹œí•˜ë©° íŒ€ì›ë“¤ì„ ì´ëŒì–´ì£¼ì—ˆëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ì˜¤íˆë ¤ ì œê°€ ë” ë§ì´ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
>
> íŠ¹íˆ íŒŒì´í”„ë¼ì¸ ì‘ì—… ì¤‘ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì¶œë ¥ìœ¼ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ë²„ê·¸ë¥¼ ë§ˆì£¼í•˜ë©° ë§ì€ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. íŒ€ì›ë“¤ì—ê²Œ ì„¤ëª…í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì´ í° ë°°ì›€ì˜ ê¸°íšŒì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
>
> ë˜í•œ, ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” Streamlitì„ í™œìš©í•˜ì—¬ ì‹œê°í™”ì™€ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ì‘ì—…ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤. ì²˜ìŒ ì˜ˆìƒë³´ë‹¤ ë‹¤ë£¨ê¸° ì‰¬ì› ê³ , í¥ë¯¸ë¡­ê²Œ ë™ì‘í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì„±ì´ ì¢‹ì•„ ìì£¼ í™œìš©í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
>
ì„ì—°ê²½
> ì €ëŠ” íŒ€ì›ë“¤ê³¼ ê°™ì´ ë¨¸ì‹ ëŸ¬ë‹ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë§¡ê³  íŒŒì´í”„ë¼ì¸ ì œì‘ì„ ìœ„í•´ ë…¸ë ¥í–ˆì§€ë§Œ ëë‚´ ì‹¤íŒ¨ í•˜ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì‹¤íŒ¨ ì†ì—ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ëŠ” ëŠì„ì—†ëŠ” ì‹œë„ì™€ í•™ìŠµì€ ì œ ì—­ëŸ‰ì„ ë” ì»¤ì§ˆ ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì£¼ì—ˆê³ , ë‹¤ìŒì—ëŠ” ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¼ ìì‹ ê°ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
>
