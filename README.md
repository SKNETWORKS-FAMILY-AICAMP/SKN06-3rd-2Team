# SKN06-3rd-2Team
ê°•ì±„ì—°, ê¹€ë™ëª…, ë°•ì°½ê·œ, ë°±í•˜ì€, í™ì¤€

## <í‘í‘ìš”ë¦¬ì‚¬>

### íŒ€ì›

| ê°•ì±„ì—° | ê¹€ë™ëª…| ë°•ì°½ê·œ | ë°±í•˜ì€| í™ì¤€ |
| --- | --- | --- | --- | --- |
| ![IE001847808_STD](https://github.com/user-attachments/assets/c2f2176b-7c40-4769-b3a4-7dbef5565109) |![ëŒì•„ì´](https://github.com/user-attachments/assets/eac6eb13-7166-409a-9f04-defaeb629cfe)|![100](https://github.com/user-attachments/assets/89f5e5ed-6412-44bf-99e9-13bef9668415)|![ê°•ë¡ì´í˜•](https://github.com/user-attachments/assets/3f3a4698-0df0-4028-8a9c-d8ced265a568)|![ëˆˆê°ì„±ì¬](https://github.com/user-attachments/assets/19d76aae-223c-44b1-9fb9-57fd0991293f)|
|ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”|ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•|RAG ì²´ì¸ ì„¤ê³„ ë° ìµœì í™”|

</br>

### RAG ê¸°ë°˜ - ë§›ì§‘ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ![small BR](https://github.com/user-attachments/assets/9d9b741c-075b-4316-aeb3-91135fc85acb)![small BR](https://github.com/user-attachments/assets/f5c8c9cf-e5a7-41ad-acf2-36e3d24f374b)![small BR](https://github.com/user-attachments/assets/516ef109-240f-4142-b0da-dbe3ed72ce9e)
### ğŸ´ ê°œë°œ ê¸°ê°„
2024.12.23 ~ 2024.12.26 (ì´ 3ì¼)
### ğŸ´ ê°œìš”
ìš°ë¦¬ë‚˜ë¼ ìµœì´ˆì˜ ë§›ì§‘ â€œë¸”ë£¨ë¦¬ë³¸â€</br>
"ë„ì‹œë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¸ê³„ì ì¸ ë§›ì§‘ ê°€ì´ë“œë¶: ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´"ì—ê²Œ **ë­ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!**</br>
ì ì´ì œ ë‚˜ë§Œì˜ ë§›ì§‘ì„ ì°¾ì•„ ë– ë‚˜ë³¼ê¹Œìš”?
### ğŸ´ ëª©í‘œ
ë¸”ë£¨ë¦¬ë³¸ ì„œë² ì´ ì›¹ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìê°€ ì°¾ëŠ” ë§›ì§‘ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ
</br>
#### ğŸ´ Requirements
pandas == 2.2.3 </br>
numpy == 1.26.4 </br>
config == 0.5.1 </br>
langchain == 0.3.13 </br>
chromadb == 0.5.23 </br>
</br></br>

## ğŸ¦´ 0. ëª¨ë¸ êµ¬ì¡°
![structure](https://github.com/user-attachments/assets/628a2d6c-4033-4640-8008-118a8e4e6dba)


</br></br>



## 1. ë°ì´í„° ì¤€ë¹„ ë° ë¶„ì„
### ğŸ£ 1) ë°ì´í„° ìˆ˜ì§‘
        import requests
        import time
        restaurants = []  # ë°ì´í„° ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±
        total_pages = 576 # ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ì„¤ì •
        url_template = "https://www.bluer.co.kr/api/v1/restaurants?page={page}&size=30&query=&foodType=&foodTypeDetail=&feature=&location=&locationDetail=&area=&areaDetail=&priceRange=&ribbonType=&recommended=false&isSearchName=false&tabMode=single&searchMode=ribbonType&zone1=&zone2=&zone2Lat=&zone2Lng="
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/hal+json",
            "x-requested-with": "XMLHttpRequest"
        }
        # í¬ë¡¤ë§ ì§„í–‰
        for page in range(total_pages):
            url = url_template.format(page=page)
            response = requests.get(url, headers=headers)
    
            if response.status_code == 200:
                data = response.json()
                restaurants.extend(data["_embedded"]["restaurants"])
                print(f"Page {page + 1}/{total_pages} collected successfully.")
            else:
                print(f"Failed to fetch page {page + 1}. Status code: {response.status_code}")
        # ê° ìš”ì²­ ì‚¬ì´ì— ì‹œê°„ ê°„ê²© ì¶”ê°€
            time.sleep(2)  # 2ì´ˆ ê°„ê²©
        # ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥
        import json
        with open("restaurants.json", "w", encoding="utf-8") as f:
            json.dump(restaurants, f, ensure_ascii=False, indent=4)
        print("ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ. JSON íŒŒì¼ë¡œ ì €ì¥ë¨.")
### ğŸ”ª 2) ë¶ˆí•„ìš” ì¹¼ëŸ¼ ì‚­ì œ ë° ì •ê·œí™”
> ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’, TMI (ìœ„ë„/ê²½ë„, ì—…ì£¼ëª…) ë“±
> 
```
# ì „ì²˜ë¦¬
import pandas as pd
from pandas import json_normalize
# JSON íŒŒì¼ ì½ê¸°
with open("restaurants.json", "r", encoding="utf-8") as f:
    restaurants = json.load(f)
# Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(restaurants)
# ê¸°ì¡´ ì „ì²˜ë¦¬ ì½”ë“œ ì ìš©
# í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
columns_to_drop = ["createdDate", "id", "timeInfo", "gps", "tags", "status", "bookStatus",
                   "buzUsername", "business", "pageView", "brandMatchStatus", "brandRejectReason",
                   "orderDescending", "foodTypeDetails", "countEvaluate", "bookmark", "features",
                   "feature107", "brandBranches", "foodTypes", "brandHead", "firstImage", "firstLogoImage"]
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])
# Nested JSON ì»¬ëŸ¼ ì •ê·œí™”
nested_columns = ["headerInfo", "defaultInfo", "statusInfo", "juso", "review", "etcInfo","_links"]
for column in nested_columns:
    if column in df.columns:
        n = pd.json_normalize(df[column])  # ì •ê·œí™”
        n.columns = [f"{column}_{subcol}" for subcol in n.columns]  # ì—´ ì´ë¦„ ì ‘ë‘ì‚¬ ì¶”ê°€
        df = pd.concat([df.drop(columns=[column]), n], axis=1)  # ê¸°ì¡´ ì—´ ì‚­ì œ í›„ ê²°í•©
# í•„ìš” ì—†ëŠ” sub_columns ì œê±°
sub_columns_to_drop = ["headerInfo_nickname", "headerInfo_year", "headerInfo_ribbonTypeByOrdinal",
                       "defaultInfo_websiteFacebook", "statusInfo_storeType", "statusInfo_openEra",
                       "statusInfo_newOpenDate", "juso_roadAddrPart2", "juso_jibunAddr", "juso_zipNo",
                       "juso_admCd", "juso_detBdNmList", "juso_zone2_1", "juso_zone2_2", "juso_map_1",
                       "juso_map_2", "review_readerReview", "review_businessReview", "review_editorReview",
                       "etcInfo_toilet", "etcInfo_toiletEtc", "etcInfo_chain", "etcInfo_close", "etcInfo_renewal",
                       "etcInfo_appYn", "etcInfo_projectNo", "etcInfo_reviewerRecommend", "etcInfo_onlySiteView",
                       "etcInfo_history", "etcInfo_mainMemo", "_links_self.href", "_links_restaurant_href",
                       "_links_restaurant_templated", "_links_childrenRestaurants.href", "_links_childrenRestaurants.templated"
                       "_links_evaluates.href", "_links_relativeBusinessOrder.href", "_links_parentRestaurant.href",
                       "_links_parentRestaurant.templated", "_links_reports.href", "_links"
                       ""]
df = df.drop(columns=[col for col in sub_columns_to_drop if col in df.columns])
# ë°ì´í„° ì €ì¥
df.to_csv("nested_all_restaurants.csv", index=False)
print("ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. CSV íŒŒì¼ ì €ì¥ë¨.")
```
### ğŸ¡ 3) í˜•ì‹ ì¼ì¹˜í™”
> ë…„ë„, ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ ê¸°ì… ì—¬ë¶€, ê²°ì¸¡ì¹˜ í‘œê¸°
>
```
import numpy as np
import re
# ë¹ˆì¹¸ì´ë‚˜ "ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°ëœ ì¹¸ì„ Noneìœ¼ë¡œ ë³€ê²½
df.replace(["", "ì—†ìŒ"], None, inplace=True)
# foodDetailTypesì˜ ë¦¬ìŠ¤íŠ¸ í•´ì œ
# ê° í–‰ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ë¼ë©´ ì´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³‘í•©)
if "foodDetailTypes" in df.columns:
    df["foodDetailTypes"] = df["foodDetailTypes"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)
if "headerInfo_ribbonType" in df.columns:
    df["headerInfo_ribbonType"] = df["headerInfo_ribbonType"].map(ribbon_mapping)
# ì›¹ì‚¬ì´íŠ¸ ì—´ í†µí•©
if "defaultInfo_website" in df.columns and "defaultInfo_websiteInstagram" in df.columns:
    def merge_websites(row):
        websites = [site for site in [row.get("defaultInfo_website"), row.get("defaultInfo_websiteInstagram")] if site]
        return ", ".join(websites) if websites else None
    df["defaultInfo_website_combined"] = df.apply(merge_websites, axis=1)
    df.drop(columns=["defaultInfo_website", "defaultInfo_websiteInstagram"], inplace=True)
    df.rename(columns={"defaultInfo_website_combined": "defaultInfo_website"}, inplace=True)
# statusInfo_openDate í˜•ì‹ ë³€í™˜ (ë…„ë„ 4ìë¦¬ë¡œ ì¶”ì¶œ í›„ "2024ë…„" í˜•ì‹ìœ¼ë¡œ í‘œê¸°)
def extract_year_with_suffix(date):
    if isinstance(date, str):
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìˆ«ì 4ìë¦¬ ì¶”ì¶œ
        match = re.search(r'\d{4}', date)
        if match:
            return f"{int(match.group(0))}ë…„"  # "2024ë…„" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return None
if "statusInfo_openDate" in df.columns:
    df["statusInfo_openDate"] = df["statusInfo_openDate"].apply(extract_year_with_suffix)
# ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
df.to_csv("cleaned_all_restaurants.csv", index=False)
print("ìˆ˜ì •ëœ ìµœì¢… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
```
### ğŸ¥© 4) ì‚°ì¶œë¬¼ ì •ë¦¬
> ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë°ì´í„° : restaurants.json </br>
> ì •ê·œí™” ë° ì „ì²˜ë¦¬ í›„ ë°ì´í„° : final_restaurant.csv </br>
</br></br>
## 2. ëª¨ë¸ë§
### ğŸ– 1) embedding_vector ìƒì„±
> ê²°ê³¼ : vector_storeì— 39074 ê°œì˜ ë¬¸ì„œ ìƒì„±
```
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥
data = pd.read_csv('data/final_restaurant.csv', low_memory=False)
# ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ë„ë¡ ë¬¸ì„œí™”
doc_list = []
for _, info in data.iterrows():
    doc_list.append(Document(page_content=str(dict(info)), metadata=dict(info)))
print(f"ì´ {len(doc_list)}ê°œì˜ ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
# Vector store ì €ì¥
embedding_model = OpenAIEmbeddings(
    model=EMBEDDING_NAME
)
# Persist directory ì—†ëŠ” ê²½ìš° ìƒì„±
if not os.path.exists(PERSIST_DIRECTORY):
    os.makedirs(PERSIST_DIRECTORY)
# ì—°ê²° + document ì¶”ê°€
vector_store = Chroma.from_documents(
    documents= doc_list,
    embedding=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)
print("vecor_storeì— ì €ì¥ì™„ë£Œ")
```
</br>

### ğŸ– 2) configë¡œë¶€í„° ì„¤ì • ê°’ ì…ë ¥
> setting
```
MODEL_NAME  = config.model_name
EMBEDDING_NAME = config.embedding_name
COLLECTION_NAME = config.collection_name
PERSIST_DIRECTORY = config.persist_directory
```
> ì €ì¥ëœ ë‚´ìš© í™•ì¸
    
```
COLLECTION_NAME = "bluer_db_openai"
PERSIST_DIRECTORY = "vector_store/chroma/bluer_db"
EMBEDDING_MODEL_NAME = "text-embedding-3-small"
embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
MODEL_NAME = 'gpt-4o-mini'
# vector store ì—°ê²°
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)
# ì €ì¥ëœ ë°ì´í„° ë‚´ìš© í™•ì¸
documents = vector_store._collection.get()['documents']
metadatas = vector_store._collection.get()['metadatas']
print(f"Documents: {documents[:5]}") 
print(f"Metadatas: {metadatas[:5]}")
print(vector_store._collection.count())
```
</br></br>
### ğŸ– 3) GPT ëª¨ë¸, Prompt, Retriever ìƒì„±
```
vector_store = Chroma(
    embedding_function=embedding_model,
    collection_name=COLLECTION_NAME,
    persist_directory=PERSIST_DIRECTORY
)
# GPT Model ìƒì„±
model = ChatOpenAI(
    model='gpt-4o',
    temperature=0 
)
retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 50,
        "fetch_k": 200,
        "lambda_mult": 0.5,
        # "filters": {"ë¦¬ë³¸ê°œìˆ˜": {"$gte": 0}}
    }
)
prompt_template = ChatPromptTemplate.from_messages([
    ("system", dedent("""
        ë‹¹ì‹ ì€ í•œêµ­ì˜ ì‹ë‹¹ì„ ì†Œê°œí•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤. 
        ë°˜ë“œì‹œ ì§ˆë¬¸ì— ëŒ€í•´ì„œ [context]ì— ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. 
        ì§ˆë¬¸ì— 'ë¦¬ë³¸ê°œìˆ˜', 'í‰ì ', 'ëª‡ ê°œ'ë¼ëŠ” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°, [context]ì—ì„œ "ë¦¬ë³¸ê°œìˆ˜" í•­ëª©ì„ í™•ì¸í•´ ë‹µë³€í•˜ì„¸ìš”.
        ë¦¬ë³¸ê°œìˆ˜ëŠ” í‰ì ê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
        [context]
        {context}
    """)),
    ("human", "{question}")
])
# Chain ìƒì„±
def content_from_doc(docs:list[Document]):
    return "\n\n".join([d.page_content for d in docs])
chain =  {'context': retriever  | RunnableLambda(content_from_doc), 'question': RunnablePassthrough()}  | prompt_template | model | StrOutputParser()
```
</br>

### ğŸ¥© 4) ì‚°ì¶œë¬¼ ì •ë¦¬
> vector_store

</br>
</br>




## ğŸ˜‹ 3. ëª¨ë¸ í‰ê°€

|ì§ˆë¬¸|ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ê·¼ì²˜ ìŒì‹ì  ì¤‘ ë¦¬ë³¸ ê°œìˆ˜ê°€ 2ê°œì¸ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤˜.|
|---|---|
|ì‘ë‹µ|ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ê·¼ì²˜ì—ì„œ ë¦¬ë³¸ê°œìˆ˜ê°€ 2ê°œì¸ ë§›ì§‘ìœ¼ë¡œëŠ” 'ì¹´ë´'ì´ ìˆìŠµë‹ˆë‹¤. ì´ê³³ì€ ì¼ì‹ì£¼ì , ì´ìì¹´ì•¼ë¡œ ì •í˜¸ì˜ ì…°í”„ì˜ ì¼ì‹ ìš”ë¦¬ë¥¼ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤. ëª¨ë‘ ì‚¬ì‹œë¯¸ë¥¼ ë¹„ë¡¯í•´ ì˜ êµ¬ìš´ ìƒì„ êµ¬ì´ ë“± ì¼í’ˆ ìš”ë¦¬ë¥¼ ë§›ë³¼ ìˆ˜ ìˆìœ¼ë©°, ì œì²  ì¬ë£Œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë©”ë‰´ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤. ë§ˆì£¼ ë³´ê³  ì•‰ëŠ” ë‹¤ì¹˜ ìë¦¬ ì™¸ì— ë³„ë„ë¡œ êµ¬ë¶„ëœ ë£¸ë„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
|ì§ˆë¬¸|í”„ë‘ìŠ¤ì‹ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”|
|ì‘ë‹µ|<img src="https://github.com/user-attachments/assets/2b3c86dd-4ee0-4776-9ed1-9445f3d06359" alt="image" width="50%">|
</br>

### 1) ğŸ¥˜ í‰ê°€ ê²°ê³¼, ê²°ê³¼ë¬¼
```
Recall: 0.66
F1 Score: 0.68
```
</br>
> evaluation_results.csv </br>


### 2) ğŸ¤” Discussion
> ë°©ë²• : ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ ì‹ë‹¹(Document)ì— ëŒ€í•œ ì •ë³´ë¥¼ ë½‘ì•„ì„œ ì˜ˆìƒ ì§ˆë¬¸ ë‹¤ì„¯ê°œë¥¼ ë§Œë“¤ì–´ ì„±ëŠ¥ í‰ê°€. ì´ 100ê°œ ( 20ê°œ ì‹ë‹¹ X 5ê°œì˜ ì§ˆë¬¸)
> ê²°ê³¼


1. íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì–´ë ¤ì›Œ í•¨.</br>
- ì—°ì¤‘ë¬´íœ´ í˜¹ì€ ë¬´íœ´ ë“±</br>
- > filter ì¶”ê°€ë¡œ ê°œì„ </br>

```
Sample 23:   
Question: ì„œì˜¤ë¦‰ë©”ì¹´ë‹¤ìŠ¬ê¸°ì˜ íœ´ë¬´ì¼ì€ ì–¸ì œì¸ê°€ìš”?   
Reference: ì—°ì¤‘ë¬´íœ´   
Response: ì£„ì†¡í•˜ì§€ë§Œ, [context]ì— ì„œì˜¤ë¦‰ë©”ì¹´ë‹¤ìŠ¬ê¸°ì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì„œì˜¤ë¦‰ë©”ì¹´ë‹¤ìŠ¬ê¸°ì˜ íœ´ë¬´ì¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì‹ë‹¹ì— ëŒ€í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.   
Context Used: íœ´ë¬´ì¼: ì—°ì¤‘ë¬´íœ´   
```

</br>

2. ë‹µì„ ì•Œê³  ìˆì§€ë§Œ, ì¸ì§€ë¥¼ ëª»í•¨.</br>

```
Sample 95:   
Question: ëŒ€ì €í• ë§¤êµ­ìˆ˜ì˜ ì£¼ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?   
Reference: ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬ ëŒ€ì €ì¤‘ì•™ë¡œ 337   
Response: ì£„ì†¡í•˜ì§€ë§Œ, [context]ì— ì œê³µëœ ì •ë³´ì—ëŠ” ëŒ€ì €í• ë§¤êµ­ìˆ˜ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬ ëŒ€ì €ì¤‘ì•™ë¡œ 337ì— ìœ„ì¹˜í•œ ëŒ€ì €í• ë§¤êµ­ìˆ˜ì˜ ì£¼ì†ŒëŠ” í•´ë‹¹ ì£¼ì†Œì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì •ë³´ëŠ” ë‹¤ë¥¸ ì¶œì²˜ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.   
Context Used: ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬ ëŒ€ì €ì¤‘ì•™ë¡œ 337   
```


</br></br>

## 5. íŒ€ì› íšŒê³      
ê°•ì±„ì—°
> í”Œì í•˜ë©´ì„œ ë§›ì§‘ ë§ì´ ì•Œê²Œë˜ì–´ì„œ ì¡°ì•—ë‹¤ ìš°í•˜í•˜
</br>

ê¹€ë™ëª…
> í¬ë¡¤ë§ì´ ì„ ë…€ì˜€ë„¤..
</br>

ë°•ì°½ê·œ
>
</br>

ë°±í•˜ì€
>
</br>

í™ì¤€
> ì–´ë””ë¡œ ê°€ì•¼í•˜ì˜¤...

